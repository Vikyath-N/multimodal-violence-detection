{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e22ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756a8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"audio_clips\"\n",
    "METADATA_CSV = \"clip_list - clip_list.csv\"  # contains filename,label\n",
    "OUTPUT_DIR = \"wav2vec2_aggression_model\"\n",
    "MODEL_CHECKPOINT = \"facebook/wav2vec2-base\"\n",
    "TARGET_SAMPLE_RATE = 16_000\n",
    "MAX_DURATION_SEC = 5               # max length for padding/truncation\n",
    "NUM_LABELS = 2                     # violent vs non-violent\n",
    "LABEL2ID = {\"Non Violent\": 0, \"Violent\": 1}\n",
    "ID2LABEL = {v: k for k, v in LABEL2ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7c9959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load dataset from CSV\n",
    "print(\"Loading metadata...\")\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=METADATA_CSV,\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6787f1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming columns...\n"
     ]
    }
   ],
   "source": [
    "# 2. Rename clip_name to filename\n",
    "print(\"Renaming columns...\")\n",
    "ds = ds.rename_column(\"clip_name\", \"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a73054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping labels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff8caca86aa433084f77a2226f7674a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Mapping labels...\")\n",
    "def map_labels(example):\n",
    "    example[\"labels\"] = LABEL2ID[example[\"class\"]]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(map_labels, remove_columns=[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb54820",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2, seed=42)\n",
    "train_ds = ds[\"train\"]\n",
    "eval_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23ea702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/USC MS CSAI Program/Multimodal Prob Learning /venv/lib/python3.10/site-packages/transformers/configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model and feature extractor...\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_CHECKPOINT, return_attention_mask=True)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    num_labels=NUM_LABELS,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9c2d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and training datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45d30c4dfd54c95b1041945b443c4b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a00f5fe94a34fe8ab9b173ffd2f0224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    # Load waveform\n",
    "    path = os.path.join(AUDIO_DIR, batch[\"filename\"])\n",
    "    waveform, sr = torchaudio.load(path)  # waveform shape: (channels, samples)\n",
    "    # If necessary, resample to target sample rate\n",
    "    if sr != TARGET_SAMPLE_RATE:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=TARGET_SAMPLE_RATE)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = TARGET_SAMPLE_RATE\n",
    "    audio = waveform[0].numpy()\n",
    "\n",
    "    # Feature extraction with padding/truncation\n",
    "    inputs = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=sr,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=sr * MAX_DURATION_SEC,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask[0]\n",
    "    return batch\n",
    "\n",
    "print(\"Preprocessing and training datasets...\")\n",
    "train_ds = train_ds.map(preprocess, remove_columns=[\"filename\"])\n",
    "eval_ds  = eval_ds.map(preprocess, remove_columns=[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd3a0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "701a5d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_25868/4128464920.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 8. TrainingArguments & Trainer setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-5,\n",
    "    do_train=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efbc019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 27:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.473700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.424800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstate_dict(), state_path)\n\u001b[1;32m      6\u001b[0m full_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggression_detector_full.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved state_dict to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved full model to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Development/USC MS CSAI Program/Multimodal Prob Learning /venv/lib/python3.10/site-packages/torch/serialization.py:944\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 944\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Development/USC MS CSAI Program/Multimodal Prob Learning /venv/lib/python3.10/site-packages/torch/serialization.py:1190\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[1;32m   1189\u001b[0m pickler \u001b[38;5;241m=\u001b[39m PyTorchPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[0;32m-> 1190\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m   1192\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m~/Development/USC MS CSAI Program/Multimodal Prob Learning /venv/lib/python3.10/site-packages/torch/nn/utils/parametrize.py:340\u001b[0m, in \u001b[0;36m_inject_new_class.<locals>.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSerialization of parametrized modules is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported through state_dict(). See:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "state_path = os.path.join(OUTPUT_DIR, \"aggression_detector_state.pt\")\n",
    "torch.save(trainer.model.state_dict(), state_path)\n",
    "full_model_path = os.path.join(OUTPUT_DIR, \"aggression_detector_full.pt\")\n",
    "torch.save(trainer.model, full_model_path)\n",
    "print(f\"Saved state_dict to {state_path}\")\n",
    "print(f\"Saved full model to {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7980d923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7800\n",
      "Test F1 Score: 0.4500\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"Test F1 Score: {metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3660a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
